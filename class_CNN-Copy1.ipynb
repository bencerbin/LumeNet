{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ac48da",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f682fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchsummary import summary\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "jtplot.style(context=\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f6e8881-fddc-412b-9e87-ed20a80721ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #makes github ignore all the data\n",
    "\n",
    "# with open(\".gitignore\", \"w\") as f:\n",
    "#     f.write(\"\"\"\n",
    "# # Ignore image data folders\n",
    "# tiny-imagenet-200/\n",
    "# tiny-imagenet-200-grayscale/\n",
    "\n",
    "# # Ignore any .DS_Store or similar files\n",
    "# .DS_Store\n",
    "\n",
    "# # Ignore Python cache files\n",
    "# __pycache__/\n",
    "# *.pyc\n",
    "# \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3126b053",
   "metadata": {},
   "source": [
    "## Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95980da-35df-4aeb-bb29-8641907d7574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89474e47-8c73-4c9b-993a-3f84954b934b",
   "metadata": {},
   "source": [
    "## Dataset Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c47113f8-7f6b-41b0-ae28-ea9c495701d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(path, batch_size, valid_batch_size=0):\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    tbs = len(train_dataset) if batch_size == 0 else batch_size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=tbs, shuffle=True)\n",
    "\n",
    "    valid_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    vbs = len(valid_dataset) if valid_batch_size == 0 else valid_batch_size\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=vbs, shuffle=True)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b52fdf66-c3b6-43a3-887d-6afa013755e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_gray(img):\n",
    "    return img.mean(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd7c5a",
   "metadata": {},
   "source": [
    "## Training Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24a9968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(mb, loader, device, model, criterion, optimizer):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    dataiterator = iter(loader)\n",
    "\n",
    "    for batch in progress_bar(range(num_batches), parent=mb):\n",
    "\n",
    "        mb.child.comment = \"Training\"\n",
    "\n",
    "        # Grab the batch of data and send it to the correct device\n",
    "        X, _ = next(dataiterator)\n",
    "        Y = X\n",
    "        Y = rgb_to_gray(Y).to(device)\n",
    "        X = X.to(device)\n",
    "        print(\"X:\", X)  # Should be [batch_size, 3, 64, 64]\n",
    "        print(\"Y:\", Y)\n",
    "\n",
    "\n",
    "        # Compute the output\n",
    "        output = model(Y)\n",
    "\n",
    "        # Compute loss\n",
    "        \n",
    "        loss = criterion(output, X)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73463196",
   "metadata": {},
   "source": [
    "## Validation Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "103de938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(mb, loader, device, model, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    num_correct = 0\n",
    "\n",
    "    num_classes = len(loader.dataset.classes)\n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "\n",
    "    N = len(loader.dataset)\n",
    "    num_batches = len(loader)\n",
    "    dataiterator = iter(loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        batches = range(num_batches)\n",
    "        batches = progress_bar(batches, parent=mb) if mb else batches\n",
    "        for batch in batches:\n",
    "\n",
    "            if mb:\n",
    "                mb.child.comment = f\"Validation\"\n",
    "\n",
    "            # Grab the batch of data and send it to the correct device\n",
    "            X, _ = next(dataiterator)\n",
    "            Y = X\n",
    "            Y = rgb_to_gray(Y).to(device)\n",
    "            X = X.to(device)\n",
    "\n",
    "            output = model(Y)\n",
    "\n",
    "            print(output.shape)\n",
    "            print(Y.shape)\n",
    "            loss = criterion(output, X)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Convert network output into predictions (one-hot -> number)\n",
    "            predictions = output.argmax(dim=1)\n",
    "\n",
    "            # Sum up total number that were correct\n",
    "            comparisons = predictions == Y\n",
    "            num_correct += comparisons.type(torch.float).sum().item()\n",
    "\n",
    "            # Sum up number of correct per class\n",
    "            for result, clss in zip(comparisons, Y):\n",
    "                class_correct[clss] += result.item()\n",
    "                class_total[clss] += 1\n",
    "\n",
    "    accuracy = 100 * (num_correct / N)\n",
    "    accuracies = {\n",
    "        clss: 100 * class_correct[clss] / class_total[clss]\n",
    "        for clss in range(num_classes)\n",
    "    }\n",
    "\n",
    "    return losses, accuracy, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db11651d",
   "metadata": {},
   "source": [
    "## Loss Plotting Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2ac9b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plots(mb, train_losses, valid_losses, epoch, num_epochs):\n",
    "\n",
    "    # Update plot data\n",
    "    max_loss = max(max(train_losses), max(valid_losses))\n",
    "    min_loss = min(min(train_losses), min(valid_losses))\n",
    "\n",
    "    x_margin = 0.2\n",
    "    x_bounds = [0 - x_margin, num_epochs + x_margin]\n",
    "\n",
    "    y_margin = 0.1\n",
    "    y_bounds = [min_loss - y_margin, max_loss + y_margin]\n",
    "\n",
    "    train_xaxis = torch.linspace(0, epoch + 1, len(train_losses))\n",
    "    valid_xaxis = torch.linspace(0, epoch + 1, len(valid_losses))\n",
    "    graph_data = [[train_xaxis, train_losses], [valid_xaxis, valid_losses]]\n",
    "\n",
    "    mb.update_graph(graph_data, x_bounds, y_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5041f60d",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e18e3af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cuda' device.\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# TODO: tune the training batch size\n",
    "train_batch_size = 128\n",
    "\n",
    "# Let's use some shared space for the data (so that we don't have copies\n",
    "# sitting around everywhere)\n",
    "data_path = \"~/data\"\n",
    "\n",
    "# Use the GPUs if they are available\n",
    "# TODO: if you run into GPU memory errors you should set device to \"cpu\" and restart the notebook\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cuda\"\n",
    "print(f\"Using '{device}' device.\")\n",
    "\n",
    "valid_batch_size = 32\n",
    "train_loader, valid_loader = get_data_loaders(data_path, train_batch_size, valid_batch_size)\n",
    "\n",
    "\n",
    "# Input and output sizes depend on data\n",
    "\n",
    "# class_names = sorted(os.listdir(grayscale_path))\n",
    "# num_classes = len(class_names)\n",
    "\n",
    "\n",
    "# print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b13ca31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grab a bunch of images and change the range to [0, 1]\n",
    "# nprint = 64\n",
    "# images = torch.tensor(train_loader.dataset.data[:nprint] / 255)\n",
    "# targets = train_loader.dataset.targets[:nprint]\n",
    "# labels = [f\"{class_names[target]:>10}\" for target in targets]\n",
    "\n",
    "# # Create a grid of the images (make_grid expects (BxCxHxW))\n",
    "# image_grid = make_grid(images.permute(0, 3, 1, 2))\n",
    "\n",
    "# _, ax = plt.subplots(figsize=(16, 16))\n",
    "# ax.imshow(image_grid.permute(1, 2, 0))\n",
    "# ax.grid(None)\n",
    "\n",
    "# images_per_row = int(nprint ** 0.5)\n",
    "# for row in range(images_per_row):\n",
    "#     start_index = row * images_per_row\n",
    "#     print(\" \".join(labels[start_index : start_index + images_per_row]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d526f6",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "289e3507-b5c1-42dd-965d-4c9a9d52bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "# Simple CNN\n",
    "class ColorizationCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorizationCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=4, dilation=2)\n",
    "        self.relu1 = nn.ReLU(True)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=4, dilation=2)\n",
    "        self.relu2 = nn.ReLU(True)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=4, dilation=2)\n",
    "        self.relu3 = nn.ReLU(True)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=4, dilation=2)\n",
    "        self.relu4 = nn.ReLU(True)\n",
    "        self.conv5 = nn.Conv2d(256, 128, kernel_size=5, stride=1, padding=4, dilation=2)\n",
    "        self.relu5 = nn.ReLU(True)\n",
    "        self.conv6 = nn.Conv2d(128, 3, kernel_size=5, stride=1, padding=4, dilation=2)\n",
    "        self.relu6 = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        x = nn.functional.relu(self.conv4(x))\n",
    "        x = nn.functional.relu(self.conv5(x))\n",
    "        x = torch.sigmoid(self.conv6(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # def __init__(self):\n",
    "    #     super().__init__()\n",
    "    #     self.encoder = nn.Sequential(\n",
    "    #         nn.Conv2d(1, 64, 3, padding=1),  # grayscale input\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.MaxPool2d(2, 2),\n",
    "    #         nn.Conv2d(64, 128, 3, padding=1),\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.MaxPool2d(2, 2)\n",
    "    #     )\n",
    "    #     self.decoder = nn.Sequential(\n",
    "    #         nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.ConvTranspose2d(64, 3, 2, stride=2),  # 3-channel output\n",
    "    #         nn.Sigmoid()  # values in [0, 1]\n",
    "    #     )\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = self.encoder(x)\n",
    "    #     x = self.decoder(x)\n",
    "    #     return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c5d34fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            1,664\n",
      "├─ReLU: 1-2                              --\n",
      "├─Conv2d: 1-3                            102,464\n",
      "├─ReLU: 1-4                              --\n",
      "├─Conv2d: 1-5                            204,928\n",
      "├─ReLU: 1-6                              --\n",
      "├─Conv2d: 1-7                            819,456\n",
      "├─ReLU: 1-8                              --\n",
      "├─Conv2d: 1-9                            819,328\n",
      "├─ReLU: 1-10                             --\n",
      "├─Conv2d: 1-11                           9,603\n",
      "├─ReLU: 1-12                             --\n",
      "=================================================================\n",
      "Total params: 1,957,443\n",
      "Trainable params: 1,957,443\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# TODO: try out different network widths and depths\n",
    "# neurons_per_hidden_layer = [1024, 512, 256]\n",
    "# layer_sizes = [num_features, *neurons_per_hidden_layer, num_classes]\n",
    "# model = NeuralNetwork(layer_sizes).to(device)\n",
    "\n",
    "# TODO: complete the CNN class in the cell above this one and then uncomment this line\n",
    "# model = CNN().to(device)\n",
    "\n",
    "# TODO: use an off-the-shell model from PyTorch\n",
    "# from torchvision.models import ...\n",
    "# model = ...\n",
    "\n",
    "# TINT TODO: make the output have 3 nodes.\n",
    "# from torchvision.models import resnet18\n",
    "# model = resnet18(num_classes=num_classes).to(device)\n",
    "\n",
    "# summary(model)\n",
    "\n",
    "#TINT\n",
    "# Instantiate the model\n",
    "model = ColorizationCNN().to(device)\n",
    "summary(model)\n",
    "# pixel-wise loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# TODO: try out different Adam hyperparameters\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46607e03-207a-4f3a-bc21-160aab2bf314",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ColorizationCNN().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ac25195-f220-478f-bdf7-500ebcf91892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single batch\n",
    "\n",
    "# gray_batch, color_batch = next(iter(train_loader))\n",
    "# gray_batch, color_batch = gray_batch.to(device), color_batch.to(device)\n",
    "# output = model(gray_batch)\n",
    "# print(\"Output shape:\", output.shape)  # Should be [batch_size, 3, 64, 64]\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd179cb",
   "metadata": {},
   "source": [
    "## Training and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcd7b615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 1, 32, 32])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m mb\u001b[38;5;241m.\u001b[39mmain_bar\u001b[38;5;241m.\u001b[39mcomment \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Loss and accuracy prior to training\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m vl, accuracy, _ \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m valid_losses\u001b[38;5;241m.\u001b[39mextend(vl)\n\u001b[1;32m     16\u001b[0m accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n",
      "Cell \u001b[0;32mIn[21], line 47\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(mb, loader, device, model, criterion)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;66;03m# Sum up number of correct per class\u001b[39;00m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m result, clss \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(comparisons, Y):\n\u001b[0;32m---> 47\u001b[0m             \u001b[43mclass_correct\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclss\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     48\u001b[0m             class_total[clss] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     50\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m (num_correct \u001b[38;5;241m/\u001b[39m N)\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "# TODO: tune the number of epochs\n",
    "num_epochs = 3\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "accuracies = []\n",
    "\n",
    "# A master bar for fancy output progress\n",
    "mb = master_bar(range(num_epochs))\n",
    "mb.names = [\"Train Loss\", \"Valid Loss\"]\n",
    "mb.main_bar.comment = f\"Epochs\"\n",
    "\n",
    "# Loss and accuracy prior to training\n",
    "vl, accuracy, _ = validate(None, valid_loader, device, model, criterion)\n",
    "valid_losses.extend(vl)\n",
    "accuracies.append(accuracy)\n",
    "\n",
    "for epoch in mb:\n",
    "\n",
    "    tl = train_one_epoch(mb, train_loader, device, model, criterion, optimizer)\n",
    "    train_losses.extend(tl)\n",
    "\n",
    "    vl, accuracy, acc_by_class = validate(mb, valid_loader, device, model, criterion)\n",
    "    valid_losses.extend(vl)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    update_plots(mb, train_losses, valid_losses, epoch, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665447d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies, '--o')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xticks(range(num_epochs+1))\n",
    "plt.ylim([0, 100])\n",
    "\n",
    "max_name_len = max(len(name) for name in class_names)\n",
    "\n",
    "print(\"Accuracy per class\")\n",
    "for clss in acc_by_class:\n",
    "    class_name = class_names[clss]\n",
    "    class_accuracy = acc_by_class[clss]\n",
    "    print(f\"  {class_name:>{max_name_len+2}}: {class_accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3681fc-9d47-45f6-b8d2-c9433b60c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trues = []\n",
    "y_preds = []\n",
    "model.to(device)\n",
    "for x, y in valid_loader:\n",
    "    y_trues.append(y.cpu())\n",
    "    y_preds.append(model(x.to(device)).argmax(dim=1).cpu())\n",
    "\n",
    "y_true = torch.hstack(y_trues)\n",
    "y_pred = torch.hstack(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8c8a494-fd98-483a-b792-98ff40b05af3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(\u001b[43my_true\u001b[49m, y_pred)\n\u001b[1;32m      2\u001b[0m ConfusionMatrixDisplay(confusion_matrix\u001b[38;5;241m=\u001b[39mcm, display_labels\u001b[38;5;241m=\u001b[39mclass_names)\u001b[38;5;241m.\u001b[39mplot();\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names).plot();\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b58043e-1b64-4f21-93e0-4c744bfbc652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Take the three outputs and reconstruct an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e62466b-91f8-4668-a6ce-27a3481fd1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e175bad-38a4-4321-8625-ec58fe3b04bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python3 (cs152)",
   "language": "python",
   "name": "cs152"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
